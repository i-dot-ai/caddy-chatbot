{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import json\n",
    "from langchain.document_loaders import AsyncHtmlLoader\n",
    "from langchain.document_transformers import BeautifulSoupTransformer, Html2TextTransformer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import pandas as pd\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls(base_url, text):\n",
    "    # Regular expression to find URLs in parentheses\n",
    "    pattern = re.compile(r'\\((.*?)\\)')\n",
    "\n",
    "    # Extract all URLs\n",
    "    urls = pattern.findall(text)\n",
    "\n",
    "    # if any urls start with a /, add the base_url\n",
    "    urls = [urljoin(base_url, url) if url.startswith('/') else url for url in urls]\n",
    "\n",
    "    # Remove any urls that don't start with http\n",
    "    urls = [url for url in urls if url.startswith('http')]\n",
    "\n",
    "    # remove duplicate urls\n",
    "    urls = list(set(urls))\n",
    "\n",
    "    return urls\n",
    "\n",
    "\n",
    "def check_if_link_in_base_domain(base_url, link):\n",
    "    \"\"\" checks if a link is in the same domain as the base url \"\"\"\n",
    "    \n",
    "    if link.startswith(base_url):\n",
    "        return link\n",
    "\n",
    "    elif not link.startswith(\"http\"):\n",
    "        return f\"{base_url}{link}\"\n",
    "    \n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW FUNCTION - SINGLE PAGE\n",
    "\n",
    "def scrape_specific_page(url):\n",
    "    \"\"\" takes a url, and returns both the html and any links on the main section of the page \"\"\"\n",
    "\n",
    "    bs_transformer = BeautifulSoupTransformer()\n",
    "\n",
    "    loader = AsyncHtmlLoader(url)\n",
    "    docs = loader.load()\n",
    "\n",
    "    soup = BeautifulSoup(docs[0].page_content)\n",
    "\n",
    "    # grab links from main section only\n",
    "    if soup.find(\"div\", id=[\"main-content\", \"cads-main-content\"]):\n",
    "        main_section_html = soup.find(\"div\", id=[\"main-content\", \"cads-main-content\"])\n",
    "    elif soup.find(\"div\", class_=[\"main-content\", \"cads-main-content\"]):\n",
    "        main_section_html = soup.find(\"div\", class_=[\"main-content\", \"cads-main-content\"])\n",
    "    else:\n",
    "        main_section_html = soup\n",
    "\n",
    "    extracted_links = bs_transformer.extract_tags(str(main_section_html), ['a'])\n",
    "    links_list = extract_urls(url, extracted_links)\n",
    "\n",
    "    site_to_html = html2text.html2text(str(main_section_html))\n",
    "\n",
    "    return links_list, site_to_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW FUNCTION - MULTIPLE PAGES\n",
    "\n",
    "def scrape_url_list(base_url, url_list, cookies=None):\n",
    "    \"\"\" takes a list of urls, and returns a dataframe of markdown and urls, as well as any links found as a list\"\"\"\n",
    "\n",
    "    bs_transformer = BeautifulSoupTransformer()\n",
    "\n",
    "    if cookies:\n",
    "        loader = AsyncHtmlLoader(url_list, header_template=cookies)\n",
    "    else:\n",
    "        loader = AsyncHtmlLoader(url_list)\n",
    "    docs = loader.load()\n",
    "\n",
    "    pages = []\n",
    "    links = []\n",
    "\n",
    "    for page in tqdm(docs):\n",
    "\n",
    "        current_url = page.metadata['source']\n",
    "        \n",
    "        # get main section of page\n",
    "        soup = BeautifulSoup(page.page_content)\n",
    "\n",
    "        if url_list == [base_url]:  # for base url (homepage), use whole page to get all links\n",
    "            main_section_html = soup\n",
    "        else:\n",
    "            if soup.find(\"div\", id=[\"main-content\", \"cads-main-content\"]):\n",
    "                main_section_html = soup.find(\"div\", id=[\"main-content\", \"cads-main-content\"])\n",
    "            elif soup.find(\"div\", class_=[\"main-content\", \"cads-main-content\"]):\n",
    "                main_section_html = soup.find(\"div\", class_=[\"main-content\", \"cads-main-content\"])\n",
    "            else:\n",
    "                main_section_html = soup\n",
    "            \n",
    "        # get links on main section of page\n",
    "        extracted_links = bs_transformer.extract_tags(str(main_section_html), ['a'])\n",
    "\n",
    "        # run extract_url on each url\n",
    "        current_page_links = extract_urls(current_url, extracted_links)\n",
    "    \n",
    "        # add links if in base domain\n",
    "        current_page_links = [link for link in current_page_links if check_if_link_in_base_domain(base_url, link)]\n",
    "\n",
    "        # add current page links to the link list\n",
    "        links += current_page_links\n",
    "        \n",
    "        # remove duplicate links\n",
    "        links = list(set(links))\n",
    "\n",
    "        # page content\n",
    "        current_page_markdown = html2text.html2text(str(main_section_html))\n",
    "        page_dict = {\n",
    "            \"source_url\": current_url,\n",
    "            \"markdown\": current_page_markdown\n",
    "        }\n",
    "        pages.append(page_dict)\n",
    "\n",
    "    # Create a dataframe with page sources & contents\n",
    "    document_df = pd.DataFrame(pages)\n",
    "\n",
    "    unique_pages = document_df.drop_duplicates(subset=['source_url']).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Number of pages scraped: {len(pages)}\")\n",
    "    \n",
    "    return unique_pages, links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 2/2 [00:00<00:00,  6.45it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 25.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages scraped: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df, links = scrape_url_list(base_url=\"https://www.citizensadvice.org.uk\", url_list=[\"https://www.citizensadvice.org.uk/benefits/benefits-introduction/what-benefits-can-i-get/\", \"https://www.citizensadvice.org.uk/decision-trees/scams/\"], cookies=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 1/1 [00:00<00:00,  3.17it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages scraped: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.citizensadvice.org.uk/advisernet/consumer/buying-or-repairing-a-car/buying-a-used-car/\"\n",
    "cookie_args={'Cookie': '.CitizensAdviceLogin=' + \"D1D4629CC80A93D71B6D36E039F6E0F6A3F905099CE0E56B1F6FC56557E5324EEE5E3ADBB10AA1C5F7BF1EA945F4ECDDD5DE61469064CBFAF83F64588F4ED4DC9924B4621FE9D74111EB8992A851F69212769D78F22DC5DF83E054B87491E2B0E75EE9B0EB6DAE8824E8DAA10C9C428792C4E468\"}\n",
    "df, links = scrape_url_list(base_url=\"https://www.citizensadvice.org.uk/advisernet/\", url_list=url, cookies=cookie_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.citizensadvice.org.uk/advisernet/consumer/buying-or-repairing-a-car/the-car-you-bought-is-still-on-hire-purchase/',\n",
       " 'https://www.citizensadvice.org.uk/advisernet/consumer/#h-buying-or-repairing-a-car',\n",
       " 'https://www.citizensadvice.org.uk/advisernet/consumer/somethings-gone-wrong-with-a-purchase/getting-your-money-back-if-you-paid-by-card-or-paypal/',\n",
       " 'https://www.citizensadvice.org.uk/advisernet/consumer/get-more-help/if-you-need-more-help/',\n",
       " 'https://www.citizensadvice.org.uk/advisernet/consumer/buying-or-repairing-a-car/motor-industry-trade-associations-and-useful-contacts/',\n",
       " 'https://www.citizensadvice.org.uk/advisernet/consumer/buying-or-repairing-a-car/problems-with-a-used-car/',\n",
       " 'https://www.citizensadvice.org.uk/advisernet/consumer/buying-or-repairing-a-car/problems-with-a-car-repair/']"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "10ds-advice-bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
