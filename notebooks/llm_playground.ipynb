{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatAnthropic\n",
    "from langchain.retrievers import AmazonKendraRetriever\n",
    "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from datetime import datetime\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores.elasticsearch import ElasticsearchStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "_core_caddy_prompt = \"\"\"\n",
    "You are a friendly and helpful AI assistant at Citizens Advice, a charity in the United Kingdom that gives advice to citizens. \\\n",
    "Advisors at Citizens Advice need to help citizens of the United Kingdom who come to Citizens Advice with a broad range of issues. \\\n",
    "Your role as an AI assistant is to help the advisors with answering the questions that are given to them by citizens. You are not a replacement for human judgement \\\n",
    "but you can help humans make more informed decisions. You are truthful and create action points for the advisor from a range of sources where you provide specific details \\\n",
    "from its context. If you don't know the answer to a question, truthfully says that you don't know, rather than making up an answer.\n",
    "\n",
    "\n",
    "Advisors will ask you to provide advice on a citizen's question which can often be cross-cutting - this means that the question will have multiple themes. \\\n",
    "It's important to understand that an issue related to a disabled person falling behind on their energy bills relates to \\\n",
    "energy, debt, benefits as well as disability-based discrimination. You must think step-by-step about the question to indetify \\\n",
    "the these present in the query and formulate your response to the advisor accordinly\n",
    "\n",
    "Unless specified otherwise, assume that the question is about a citizen in England.\n",
    "\n",
    "Human: Here are a few documents in <documents> tags:\n",
    "<documents>\n",
    "{context}\n",
    "</documents>\n",
    "Based on the above documents, provide a detailed answer for, {question}. Be concise in your response and make sure to include reference to any location names \\\n",
    "stated in the question, and make sure your answer is relevant to the laws and rules of the location specified in the question.\n",
    "\n",
    "If the question discusses 'my client', your answer should refer to 'your client'. \\\n",
    "In your answer, refer to the documents you use as \"information\" rather than \"documents\". \\\n",
    "DO NOT CITE THE URL OF THE DOCUMENTS IN YOUR ANSWER.\n",
    "\n",
    "If information is needed to definitively answer the question, list a step by step set of questions that the adviser should ask the client to find out this missing information. \\\n",
    "And use language like 'could be' instead if 'is' - in the list of questions, use simple language.\n",
    "\n",
    "Use <b>bold</b> to highlight the most question-relevant parts in your response.\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    "\n",
    "CORE_PROMPT = PromptTemplate(\n",
    "    template=_core_caddy_prompt,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "from langchain.retrievers.merger_retriever import MergerRetriever\n",
    "from typing import Any\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "\n",
    "class LLMPriorityRetriever(BaseRetriever):\n",
    "    \"\"\"Retriever that merges the results of multiple retrievers.\"\"\"\n",
    "\n",
    "    retriever_list: List[BaseRetriever]\n",
    "    llm: BaseChatModel\n",
    "    alternative_retriever: BaseRetriever\n",
    "    max_document_length: int = 500\n",
    "    max_retrieved_documents: int = 6\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "            lotr = MergerRetriever(retrievers=self.retriever_list)\n",
    "            all_relevant_docs = lotr.get_relevant_documents(query)\n",
    "\n",
    "            all_docs = [(index, document.page_content[:self.max_document_length]) if len(document.page_content) > self.max_document_length else (index, document.page_content) for index, document in enumerate(all_relevant_docs)]\n",
    "\n",
    "            document_prioritisation_prompt = f\"\"\"Please read the documents below, and rank them in order of relevance to this query: '{query}'. Please pick the documents based off the fact that all queries are related to Citizen's Advice centers in England; you can use both the context of the document as well as the URL to deduce whether the document is relevant. Please rank them in order of relevance, with 1 being the most relevant, and 5 being the least relevant. Please separate your rankings with a comma, in the format of a Python list. For example, if you think document 1 is the most relevant, and document 5 is the least relevant, please enter: [1, 2, 3, 4, 5]. Return only the list with no other output.\n",
    "\n",
    "            Documents: {all_docs}\n",
    "\n",
    "            Remember to return only your list, with no other output or context.\"\"\"\n",
    "\n",
    "            llm_priority = self.llm.predict(document_prioritisation_prompt)\n",
    "\n",
    "            response_as_list = eval(llm_priority)\n",
    "\n",
    "            try:\n",
    "                 # get the top 5 documents\n",
    "                top_docs = [all_relevant_docs[index] for index in response_as_list[:self.max_retrieved_documents]]\n",
    "\n",
    "            except:\n",
    "                 # if it fails, return all the docs\n",
    "                top_docs = self.alternative_retriever.get_relevant_documents(query)\n",
    "\n",
    "            return top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.merger_retriever import MergerRetriever\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain.retrievers.merger_retriever import MergerRetriever\n",
    "from langchain_community.document_transformers import (\n",
    "    EmbeddingsClusteringFilter,\n",
    "    EmbeddingsRedundantFilter,\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "def build_chain():\n",
    "    anthropic_key = \"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\"\n",
    "    opensearch_index = \"caddy_vector_index\"\n",
    "    opensearch_https = \"https://search-caddy-vector-db-qocexipbio42soi53zdl5zzcqy.aos.eu-west-2.on.aws\"\n",
    "    opensearch_admin = \"caddyKing\"\n",
    "    opensearch_password = \"C4ddyV3ct0rK1ng!\"\n",
    "\n",
    "    llm = ChatAnthropic(\n",
    "        temperature=0.2,\n",
    "        max_tokens=500,\n",
    "        anthropic_api_key=anthropic_key,\n",
    "        verbose=True\n",
    "        )\n",
    "    \n",
    "    auth = (opensearch_admin, opensearch_password) # For testing only. Don't store credentials in code.\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "    \n",
    "    vectorstore = OpenSearchVectorSearch(\n",
    "        index_name=opensearch_index,\n",
    "        opensearch_url=opensearch_https,\n",
    "        http_auth = auth,\n",
    "        embedding_function=embeddings,\n",
    "        \n",
    "    )\n",
    "    \n",
    "    advisernet_retriever = vectorstore.as_retriever(\n",
    "        k='5',\n",
    "        strategy=ElasticsearchStore.ApproxRetrievalStrategy(\n",
    "        hybrid=True),\n",
    "        search_kwargs={\n",
    "            'filter': {\n",
    "                'match': {\n",
    "                    'metadata.domain_description': 'AdvisorNet'\n",
    "                    }\n",
    "                }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    gov_retriever = vectorstore.as_retriever(\n",
    "        k='5',\n",
    "        strategy=ElasticsearchStore.ApproxRetrievalStrategy(\n",
    "        hybrid=True),\n",
    "        search_kwargs={\n",
    "            'filter': {\n",
    "                'match': {\n",
    "                    'metadata.domain_description': 'GOV.UK'\n",
    "                    }\n",
    "                }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    ca_retriever = vectorstore.as_retriever(\n",
    "        k='5',\n",
    "        strategy=ElasticsearchStore.ApproxRetrievalStrategy(\n",
    "        hybrid=True),\n",
    "        search_kwargs={\n",
    "            'filter': {\n",
    "                'match': {\n",
    "                    'metadata.domain_description': 'Citizens Advice'\n",
    "                    }\n",
    "                }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    lotr = MergerRetriever(retrievers=[gov_retriever, advisernet_retriever, ca_retriever])\n",
    "\n",
    "    filter_ordered_by_retriever = EmbeddingsClusteringFilter(\n",
    "        embeddings=embeddings,\n",
    "        num_clusters=3,\n",
    "        num_closest=2,\n",
    "        sorted=True,\n",
    "        remove_duplicates=True\n",
    "    )\n",
    "\n",
    "    pipeline = DocumentCompressorPipeline(transformers=[filter_ordered_by_retriever])\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=pipeline, base_retriever=lotr\n",
    "    )\n",
    "\n",
    "    claude_llm = ChatAnthropic(\n",
    "        temperature=0.2,\n",
    "        max_tokens=500,\n",
    "        anthropic_api_key=anthropic_key,\n",
    "        verbose=True\n",
    "        )\n",
    "    \n",
    "    claude_priority_retriever =  LLMPriorityRetriever(\n",
    "        retriever_list=[gov_retriever, advisernet_retriever, ca_retriever],\n",
    "        llm=claude_llm,\n",
    "        alternative_retriever=compression_retriever)\n",
    "\n",
    "    chain = RetrievalQA.from_chain_type(\n",
    "        llm=claude_llm,\n",
    "        # retriever=claude_priority_retriever,\n",
    "        # retriever=lotr,\n",
    "        retriever=compression_retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\n",
    "         \"prompt\":CORE_PROMPT,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    ai_prompt_timestamp = datetime.now()\n",
    "    return chain, ai_prompt_timestamp\n",
    "\n",
    "def run_chain(chain, prompt: str, history:[]):\n",
    "    ai_response = chain({\"query\": prompt, \"chat_history\": history})\n",
    "    ai_response_timestamp = datetime.now()\n",
    "\n",
    "    return ai_response, ai_response_timestamp\n",
    "\n",
    "chain, ai_prompt_timestam = build_chain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask Caddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_response, ai_response_timestamp = run_chain(\n",
    "    chain, \n",
    "    prompt=\"My client has moved into a leasehold flat and has to pay service charges of £40 per week. They receive Universal Credit, can they get help to pay the charges?\", \n",
    "    history=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_count = {}\n",
    "\n",
    "for source in ai_response['source_documents']:\n",
    "    if source.metadata['domain_description'] in document_count:\n",
    "        document_count[source.metadata['domain_description']] += 1\n",
    "    else:\n",
    "        document_count[source.metadata['domain_description']] = 1\n",
    "\n",
    "for item, count in document_count.items():\n",
    "    print(f\"{item}: {count} documents\")\n",
    "\n",
    "for document in ai_response['source_documents']:\n",
    "    print(f\"{document.metadata['source_url']} | characters = {format(len(document.page_content), ',')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(ai_response['result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_response['source_documents'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opensearch_index = \"caddy_vector_index\"\n",
    "opensearch_https = \"https://search-caddy-vector-db-qocexipbio42soi53zdl5zzcqy.aos.eu-west-2.on.aws\"\n",
    "opensearch_admin = \"caddyKing\"\n",
    "opensearch_password = \"C4ddyV3ct0rK1ng!\"\n",
    "\n",
    "auth = (opensearch_admin, opensearch_password) # For testing only. Don't store credentials in code.\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "vectorstore = OpenSearchVectorSearch(\n",
    "    index_name=opensearch_index,\n",
    "    opensearch_url=opensearch_https,\n",
    "    http_auth = auth,\n",
    "    embedding_function=embeddings,\n",
    "    \n",
    ")\n",
    "\n",
    "docs = vectorstore.similarity_search(query=\"what can I do if I'm being evicted?\", k=10)\n",
    "\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.client.search(index='*', body={\"query\": {\"match_all\": {}}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectorstore.max_marginal_relevance_search(\n",
    "    query=\"what can I do if I'm being evicted?\", \n",
    "    k=4, \n",
    "    fetch_k=20, \n",
    "    lambda_param=0.5, \n",
    "    # metadata_field={\n",
    "    #     # 'domain_description': 'AdvisorNet',\n",
    "    #     'domain_description': 'Citizens Advice'\n",
    "    # },\n",
    "    metadata_field=\"domain_description\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opensearch_https = \"https://search-caddy-vector-db-qocexipbio42soi53zdl5zzcqy.aos.eu-west-2.on.aws\"\n",
    "opensearch_admin = \"caddyKing\"\n",
    "opensearch_password = \"C4ddyV3ct0rK1ng!\"\n",
    "opensearch_index = \"caddy_vector_index\"\n",
    "\n",
    "auth = (opensearch_admin, opensearch_password) # For testing only. Don't store credentials in code.\n",
    "\n",
    "vectorstore = OpenSearchVectorSearch(\n",
    "    index_name=opensearch_index,\n",
    "    opensearch_url=opensearch_https,\n",
    "    http_auth = auth,\n",
    "    embedding_function=embeddings,\n",
    "    \n",
    ")\n",
    "\n",
    "result = vectorstore.similarity_search(\"Hello\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(docs):\n",
    "    print(i, d.metadata['domain_description'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
